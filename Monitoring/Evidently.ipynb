{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a949573",
   "metadata": {},
   "source": [
    "# Evidently \n",
    "\n",
    "Once your machine learning model is deployed in production, it needs to be monitored. This is done to catch possible problems and faults with and in the machine learning model as quickly as possible. One of the aspects that should be monitored is the performance of the machine learning model itself. This includes the input data, the model performance and possible drift that has happened in the data or predictions. There are several tools availble to help you with monitoring. One of the best open-source tools is Evidently, a tool which can be easily implemented using its python library. It creates test suites and performance reports using presets which can then be used however you want. In this notebook we will see the Evidently basics. Therefore, the first thing to do will be to import all libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b07e1e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laral\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\evidently\\core\\metric_types.py:375: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
      "  np_bool = np.bool  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from scipy.stats import wasserstein_distance, kstest, chi2_contingency\n",
    "\n",
    "from evidently import DataDefinition\n",
    "from evidently import Dataset\n",
    "from evidently import Report\n",
    "from evidently.presets import DataSummaryPreset\n",
    "from evidently.presets import DataDriftPreset\n",
    "from evidently.presets import ClassificationPreset\n",
    "from evidently.tests import *\n",
    "from evidently.metrics import *\n",
    "from evidently import BinaryClassification\n",
    "\n",
    "from utils.data_utils import Data_utils\n",
    "from utils.model import NN_model\n",
    "from utils.predict import evaluate_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675506f8",
   "metadata": {},
   "source": [
    "## Prepare dataset\n",
    "To start, we will use our water_potability dataset. However, we have split it up into two parts: a reference dataset and a current dataset. We have trained our model on the reference datasest. The current dataset is new, possibly drifted data which would normally arrive batch by batch or sample by sample from the sensors. With Evidently you can use the reference dataset to compare the current dataset to. When these datasets deviate from each other in the performed tests, an error will be given. First let's load our dataset into python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64da666",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_reference = pd.read_csv(\"dataset/dataset_reference.csv\", delimiter=\",\") \n",
    "dataframe_current = pd.read_csv(\"dataset/dataset_current.csv\", delimiter=\",\")\n",
    "print(\"reference data: \" + str(dataframe_reference.head()))\n",
    "print(\"current data: \" + str(dataframe_current.head()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd83817d",
   "metadata": {},
   "source": [
    "Next, we are going to run the model on the given datasets and then add the resulting predictions to this datasets. Remember that the model has been trained on the reference dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a024ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149/149 [00:00<00:00, 13249.48it/s]\n",
      "100%|██████████| 262/262 [00:00<00:00, 13345.49it/s]\n"
     ]
    }
   ],
   "source": [
    "data = Data_utils()\n",
    "# load our data with a batch size of 8\n",
    "data.load_train_data(8)\n",
    "data.load_test_data(8)\n",
    "\n",
    "# create a neural network Class and load the weights of a previously trained model.\n",
    "model_params = {\n",
    "            \"n_layers\": 3,\n",
    "            \"dimensions\": [54, 95, 95],\n",
    "        }\n",
    "model = NN_model(model_params) \n",
    "model.load_state_dict(torch.load(\"classification_model.pth\"))\n",
    "\n",
    "# Evaluate our reference data on the machine learning model and add the predictions to the dataframe\n",
    "predictions_reference = evaluate_model(model, data.train_dataloader)\n",
    "\n",
    "predictions_ref = torch.cat(predictions_reference).tolist()\n",
    "predictions_ref = [int(x) for x in predictions_ref]\n",
    "dataframe_reference[\"prediction\"] = predictions_ref\n",
    "\n",
    "# Evaluate our predictions data on the machine learning model and add the predictions to the dataframe\n",
    "predictions_current = evaluate_model(model, data.cur_dataloader)\n",
    "predictions = torch.cat(predictions_current).tolist()\n",
    "predictions = [int(x) for x in predictions]\n",
    "dataframe_current[\"prediction\"] = predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54358924",
   "metadata": {},
   "source": [
    "Now that we have added our predictions to our dataframe we are going to change the naming of our dataframe to correspond to the terms that Evidently knows. Our true labels, which corresponds to the potability are changed to be named \"target\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a596c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1186\n",
      "            ph  Hardness    Solids  Chloramines   Sulfate  Conductivity  \\\n",
      "0     0.265850  0.591734  0.295897     0.399690  0.480143      0.834554   \n",
      "1     0.675542  0.964702  0.285135     0.391213  0.370691      0.440050   \n",
      "2     0.509336  0.516429  0.224086     0.511483  0.217911      0.421722   \n",
      "3     0.543586  0.852255  0.524375     0.487242  0.431409      0.494766   \n",
      "4     0.525234  0.788631  0.297562     0.109985  0.268350      0.337211   \n",
      "...        ...       ...       ...          ...       ...           ...   \n",
      "1181  0.689193  0.965843  0.261366     0.436726  0.367388      0.621164   \n",
      "1182  0.509336  0.630085  0.136719     0.628364  0.480143      0.500508   \n",
      "1183  0.620726  0.877993  0.229659     0.486982  0.480143      0.574168   \n",
      "1184  0.858343  0.341949  0.602661     0.651018  0.182131      0.524201   \n",
      "1185  0.700483  0.926171  0.535994     0.468070  0.480143      0.508279   \n",
      "\n",
      "      Organic_carbon  Trihalomethanes  Turbidity  prediction  target  \n",
      "0           0.547991         0.490667   0.576793           0       0  \n",
      "1           0.353665         0.275912   0.496327           0       0  \n",
      "2           0.415250         0.469482   0.405562           0       0  \n",
      "3           0.569818         0.688474   0.418282           0       0  \n",
      "4           0.588207         0.695189   0.377341           0       0  \n",
      "...              ...              ...        ...         ...     ...  \n",
      "1181        0.443279         0.581863   0.545274           0       1  \n",
      "1182        0.198622         0.653463   0.425328           0       1  \n",
      "1183        0.328166         0.332729   0.653499           0       1  \n",
      "1184        0.601257         0.360297   0.551950           1       1  \n",
      "1185        0.325807         0.609967   0.349570           0       1  \n",
      "\n",
      "[1186 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "dataframe_reference[\"target\"] = dataframe_reference[\"Potability\"].astype(int)\n",
    "dataframe_reference = dataframe_reference.drop([\"Potability\"], axis=1)\n",
    "print(dataframe_reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c87bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             ph    Hardness        Solids  Chloramines     Sulfate  \\\n",
      "0           NaN  204.890455  20791.318981     7.300212  368.516441   \n",
      "1      8.099124  224.236259  19909.541732     9.275884         NaN   \n",
      "2      8.316766  214.373394  22018.417441     8.059332  356.886136   \n",
      "3      5.584087  188.313324  28748.687739     7.544869  326.678363   \n",
      "4     10.223862  248.071735  28749.716544     7.513408  393.663396   \n",
      "...         ...         ...           ...          ...         ...   \n",
      "2085   6.069616  186.659040  26138.780191     7.747547  345.700257   \n",
      "2086   4.668102  193.681735  47580.991603     7.166639  359.948574   \n",
      "2087   7.808856  193.553212  17329.802160     8.061362         NaN   \n",
      "2088   5.126763  230.603758  11983.869376     6.303357         NaN   \n",
      "2089   7.874671  195.102299  17404.177061     7.509306         NaN   \n",
      "\n",
      "      Conductivity  Organic_carbon  Trihalomethanes  Turbidity  prediction  \\\n",
      "0       564.308654       10.379783        86.990970   2.963135           1   \n",
      "1       418.606213       16.868637        66.420093   3.055934           1   \n",
      "2       363.266516       18.436524       100.341674   4.628771           1   \n",
      "3       280.467916        8.399735        54.917862   2.559708           1   \n",
      "4       283.651634       13.789695        84.603556   2.672989           0   \n",
      "...            ...             ...              ...        ...         ...   \n",
      "2085    415.886955       12.067620        60.419921   3.669712           1   \n",
      "2086    526.424171       13.894419        66.687695   4.435821           0   \n",
      "2087    392.449580       19.903225              NaN   2.798243           1   \n",
      "2088    402.883113       11.168946        77.488213   4.708658           0   \n",
      "2089    327.459760       16.140368        78.698446   2.309149           0   \n",
      "\n",
      "      target  \n",
      "0          0  \n",
      "1          0  \n",
      "2          0  \n",
      "3          0  \n",
      "4          0  \n",
      "...      ...  \n",
      "2085       1  \n",
      "2086       1  \n",
      "2087       1  \n",
      "2088       1  \n",
      "2089       1  \n",
      "\n",
      "[2090 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "dataframe_current[\"target\"] = dataframe_current[\"Potability\"].astype(int)\n",
    "dataframe_current = dataframe_current.drop([\"Potability\"], axis=1)\n",
    "print(dataframe_current)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f3e7fc",
   "metadata": {},
   "source": [
    "It is also good practice to create an Evidently dataset version of your pandas dataframes. To be able to use these datasets, you have to create an Evidently data definition of your dataset. Then you can add this definition to the Evidently dataset This can be done as follows:</br>\n",
    "\n",
    "    data_definition = DataDefinition(\n",
    "        classification= [BinaryClassification|MulticlassClassification(\n",
    "            target = \"target\",\n",
    "            prediction_labels = \"prediction\",\n",
    "            )],\n",
    "            numerical_columns=[\"name\", \"of\", \"columns\"],\n",
    "            categorical_columns=[\"name\", \"of\", \"columns\"],\n",
    "            datetime_columns=[\"name\", \"of\", \"columns\"],\n",
    "            text_columns=[\"name\", \"of\", \"columns\"],\n",
    "    )\n",
    "    dataset = Dataset.from_pandas(dataframe, data_definition)\n",
    "\n",
    "Do this for both datasets.\n",
    "\n",
    "Note that in data quality and drift detection reports you can also use normal dataframes. However for model quality reports, where we do calculations using the labels and predictions, Evidently needs to know which columns correspond with that information. A second note to make here is that telling which columns have which types is optional but can help Evidently with creating better reports and tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54609871",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea8c9025",
   "metadata": {},
   "source": [
    "### Data Quality check\n",
    "\n",
    "The first thing you should check is the quality of your current dataset. This can be done on it's own, like checking the ratio of null-values, or this can be done in comparison to a reference dataset, like comparing the input shapes. With Evidently you can easily implement a report of the data quality by using the <code>DataSummaryPreset()</code>. To create a report, you first define it and then run it with the wanted data. Afterwards you can save your report as an HTML file, a JSON file or a python dictionary.\n",
    "</br>\n",
    "<code>\n",
    "report = Report([<i>preset_funtion(),</i>]) </br>\n",
    "my_eval = report.run(current_dataset, reference_dataset|None)</br></br>\n",
    "my_eval.save_html(\"path/to/html\")</br>\n",
    "my_eval.save_json(\"path/to/json\")</br>\n",
    "my_eval.dict()\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0832db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ace8086",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b438bcc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38e2a496",
   "metadata": {},
   "source": [
    "When comparing the reference and current dataset. What do you see? What are the main differences? Are there data cleaning steps that are missed in the current data? you can use the Data exercises to help you clean the data where needed. If you are going to clean the data you are going to have to rerun the model on the cleaned data instead of the uncleaned one. This because we know that the model has been trained on clean data, which would make the predictions of unclean data wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948c8675",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707cbf07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39262e4b",
   "metadata": {},
   "source": [
    "Now that you cleaned your current dataset, which other difference do you see between reference and current dataset? Do you suspect drift?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d409d99",
   "metadata": {},
   "source": [
    "## Model Quality Check\n",
    "Next to your data quality, it is also a good idea to check your model quality. This includes aspects like creating a confusion matrix and plotting the ROC curve.\n",
    "In Evidently you again have a preset for this. It is split up in <code>ClassificationPreset()</code> for classification tasks and <code>RegressionPreset()</code> for regression tasks. To create, call and save your report, you can use the same steps as the <code>DataSummaryPreset()</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2521c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "244a8dac",
   "metadata": {},
   "source": [
    "With Evidently you can also combine presets. When you create your report, you just have to add both presets.\n",
    "</br>\n",
    "<code>\n",
    "report = Report([<i>Preset_1(), preset_2()</i>])\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e2d584",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07999e7b",
   "metadata": {},
   "source": [
    "Next to combining presets, you can also add tests to your report. These are tests that Evidently will do for you and show their results. For example having the accuracy be above a certain value. Evidently has certain tests presets which you can activate by setting the <code>include_tests</code> parameter to <code>True</code> in the Report function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9bd367",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4bd7c67",
   "metadata": {},
   "source": [
    "## Drift detection\n",
    "\n",
    "The previous part assumed that you have true labels available of your predictions. However, this is most of the time not the case. Therefore we detect drift in the data and predictions of the machine learning model. Here we run metrics like the wasserstein distance between the datasets to know if the distribution has changed significantly. The function needed for this is the <code>DataDriftPreset()</code>. Again, tests can be added by setting the <code>include_tests</code> parameter to <code>True</code>. Here both the current and reference dataset are needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6e8c87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a06cd822",
   "metadata": {},
   "source": [
    "you can also customize your report. For example, you can decide which columns to detect the drift on, or which tests to use. You can also change the tresholds and other parameters of the tests.</br>\n",
    "\n",
    "<b>Limit columns:</b>\n",
    "\n",
    "    DataDriftPreset(column=[\"list\",\"of\",\"columns\"])\n",
    "<b>Choose metrics:</b>\n",
    "\n",
    "    report = Report([\n",
    "        Metric_1(),\n",
    "        Metric_2(column=[\"list\", \"of\", \"columns\"], parameter=\"value\")\n",
    "    ])\n",
    "A list of metrics can be found [here](https://docs.evidentlyai.com/metrics/all_metrics). As you can see in the example, some metrics have their own parameters. This can for example be the method used to calculate the metric (wasserstein, psi, ks, ...).\n",
    "\n",
    "<b>Exclude tests:</b></br>\n",
    "    report = Report([\n",
    "        Metric_1(column=\"column\", tests=[]),\n",
    "        Metric_2(column=\"column\"),\n",
    "    ], \n",
    "    include_tests=True)\n",
    "\n",
    "<b>custom test conditions</b> (use eq (equal), gt(greater than), lt (less than)):\n",
    "\n",
    "    report = Report([\n",
    "        Metric_1(column=\"column\", tests=[eq(0)]),\n",
    "        Metric_2(column=\"column\", tests=[gte(18), lt(35)]),\n",
    "        Metric_3(column=\"column\", tests=[gte(Reference(relative=0.1))]),\n",
    "        Metric_4(column=\"column\", tests=[lte(Reference(absolute=10))]),\n",
    "    ])\n",
    "more information about the tests can be found [here](https://docs.evidentlyai.com/docs/library/tests).</br>\n",
    "Create your own custom evidently report. Play around with the different metrics and tests. More information about evidently can be found on their [website](https://docs.evidentlyai.com/docs/library/overview)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f29e1e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
