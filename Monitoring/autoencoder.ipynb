{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce765307",
   "metadata": {},
   "source": [
    "# Autoencoder\n",
    "\n",
    "A good way to detect drift in time series data is to use an autoencoder. An autoencoder is a machine learning model which tries to reproduce its inputs. It will do this by first reducing the inputs to a smaller representation trough an encoder. Afterwards a decoder will reconstruct the input from this smaller representation. This means that when the autoencoder is trained on a certain dataset, it can reproduce those inputs well, but when data data distribution has drifted, it will not be able to reproduce it's inputs anymore. This way we can detect when there is drift. \n",
    "\n",
    "<img src=\"images/autoencoder.png\"   />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0a3d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.data_utils_ae import split_in_sequences, create_dataloader, split_train_test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c2df73",
   "metadata": {},
   "source": [
    "## Model definition\n",
    "You can create your autoencoder model like any other pytorch model. The only difference is that the dimension size first reduces and than grows again to its original size. A lot of people however, also split the autoencoder into the encoder and the decoder part. So first we define our encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca39824",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, seq_len):\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.layer1 = nn.Linear(self.seq_len,[fill_in], bias=True)\n",
    "        nn.init.kaiming_normal_(self.layer1.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.layer2 = nn.Linear([fill_in],[fill_in], bias=True)\n",
    "        nn.init.kaiming_normal_(self.layer2.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.layer3 = nn.Linear([fill_in],[fill_in], bias=True)\n",
    "        nn.init.kaiming_normal_(self.layer3.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.layer4 = nn.Linear([fill_in],[fill_in], bias=True)\n",
    "        nn.init.kaiming_normal_(self.layer4.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
    "        self.act4 = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        x = self.act1(self.layer1(input_tensor))\n",
    "        x = self.act2(self.layer2(x))\n",
    "        x = self.act3(self.layer3(x))\n",
    "        x = self.act4(self.layer4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf121f28",
   "metadata": {},
   "source": [
    "Then we define our decoder. Maker sure the sizes of the hidden layers are the same as the sizes of the encoder reversed. The first layer of the decoder is the embedding dimension, it is a linear layer that has the same input and output size, namely the embedding dimension size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c88dff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, seq_len):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.layer0 = nn.Linear([fill_in],[fill_in], bias=True)\n",
    "        nn.init.kaiming_normal_(self.layer0.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
    "        self.act0 = nn.ReLU()\n",
    "        self.layer1 = nn.Linear([fill_in],[fill_in], bias=True)\n",
    "        nn.init.kaiming_normal_(self.layer1.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.layer2 = nn.Linear([fill_in],[fill_in], bias=True)\n",
    "        nn.init.kaiming_normal_(self.layer2.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.layer3 = nn.Linear([fill_in],[fill_in], bias=True)\n",
    "        nn.init.kaiming_normal_(self.layer3.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.layer4 = nn.Linear([fill_in],self.seq_len, bias=True)\n",
    "        nn.init.kaiming_normal_(self.layer4.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
    "        self.act4 = nn.ReLU()\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        x = self.act0(self.layer0(input_tensor))\n",
    "        x = self.act1(self.layer1(x))\n",
    "        x = self.act2(self.layer2(x))\n",
    "        x = self.act3(self.layer3(x))\n",
    "        x = self.act4(self.layer4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958d476b",
   "metadata": {},
   "source": [
    "Lastly we bring the two together in one class, the autoencoder. Here we also define the loss function. Choose a good autoencoder loss function. As help you can look at this [page](https://www.geeksforgeeks.org/loss-functions-in-deep-learning/). Once you have chosen your loss function you can add it in the fill_in_loss using the torch.nn function [page](https://docs.pytorch.org/docs/stable/nn.html#loss-functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb114a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, train_params, seq_len):\n",
    "\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "\n",
    "        self.n_epochs = train_params[\"epochs\"]\n",
    "        self.batch_size = train_params[\"batch_size\"]\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.criterion = [fill_in_loss]\n",
    "\n",
    "        self.encoder = Encoder(seq_len)\n",
    "        self.decoder = Decoder(seq_len)\n",
    "\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        \n",
    "        x = self.encoder(input_tensor)\n",
    "        x = self.decoder(x)\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365ea7f3",
   "metadata": {},
   "source": [
    "Next we define our train function. This is a standard pytorch trainingloop. The only difference is that our inputs are also our true labels in the loss function and other evaluation metric calculation. Choose a good evaluation metric to use next to your loss function to evaluate your predictions on. Again, [here](https://www.geeksforgeeks.org/regression-metrics/) is a page that can help you. Once you have chosen the metric import it from the sklearn metrics and add it to the train_model function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7489656f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f973f2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataset, val_dataset, train_params):\n",
    "    \"\"\"Trains the machine learning model. Returns the loss and evaluation metric history of the training \n",
    "    and the model with the best validation loss.\"\"\"\n",
    "    best_loss = np.inf\n",
    "    best_model = None\n",
    "    history = dict(train_loss=[], val_loss=[], train_metric=[], val_metric=[])\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=train_params[\"learning_rate\"], weight_decay=1e-8)\n",
    "    criterion = model.criterion\n",
    "    # train the model. Run the model on the inputs, calculate the losses, do backpropagation\n",
    "    for epoch in range(1, train_params[\"epochs\"]):\n",
    "        model = model.train()\n",
    "        train_losses = []\n",
    "        train_mses = []\n",
    "    \n",
    "        for (seq_true,) in train_dataset:\n",
    "            optimizer.zero_grad()\n",
    "            seq_pred = model(seq_true)\n",
    "\n",
    "            loss = criterion(seq_pred, seq_true)\n",
    "            loss.backward()\n",
    "            metric = [choose_a_metric]\n",
    "\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "            train_mses.append(metric)\n",
    "\n",
    "        model = model.eval()\n",
    "        val_losses = []\n",
    "        val_mses = []\n",
    "        # run the model and loss on the validation function\n",
    "        with torch.no_grad():\n",
    "            for (seq_true,) in val_dataset:\n",
    "                seq_pred = model(seq_true)\n",
    "\n",
    "                loss = criterion(seq_pred, seq_true)     \n",
    "                metric = [choose_a_metric]\n",
    "\n",
    "                val_losses.append(loss.item())\n",
    "                val_mses.append(metric)\n",
    "                \n",
    "        # get the losses and mse from the epoch\n",
    "        train_loss = np.mean(train_losses)\n",
    "        val_loss = np.mean(val_losses)\n",
    "        train_mse = np.mean(train_mses)\n",
    "        val_mse = np.mean(val_mses)\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_mse'].append(train_mse)\n",
    "        history['val_mse'].append(val_mse)\n",
    "\n",
    "        # decide if this version of the model is the best\n",
    "        loss = float(val_loss)\n",
    "        if loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = model\n",
    "\n",
    "        text = f'Epoch = {epoch}, train loss = {train_loss}, val loss = {val_loss}'\n",
    "        print(text)\n",
    "\n",
    "    return history, best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56890f20",
   "metadata": {},
   "source": [
    "Lastly, before we start with our dataset, we will also define a predict function which will input our data and return the predictions of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd585dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dataset):\n",
    "    \"\"\"Runs the given dataset on the given model. It returns the predictions, losses, \n",
    "    evaluation metrics and the original input vlaues\"\"\"\n",
    "    predictions, input_values, losses, eval_metrics = [], [], [], []\n",
    "    criterion = model.criterion\n",
    "    model = model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (seq_true,) in dataset:\n",
    "            # seq_true = torch.Tensor(seq_true)\n",
    "            seq_pred = model(seq_true)\n",
    "            loss = criterion(seq_pred, seq_true)\n",
    "                \n",
    "            seq_true_metric = np.array([item for row in seq_true.tolist() for item in row])\n",
    "            seq_pred_metric = np.array([item for row in seq_pred.tolist() for item in row])\n",
    "            difference = np.mean(np.abs(seq_true_metric - seq_pred_metric))\n",
    "\n",
    "            # flatten all data to be able to use as an array\n",
    "            predictions.append(seq_pred.numpy().flatten())\n",
    "            input_values.append(seq_true.numpy().flatten())\n",
    "            losses.append(loss.item())\n",
    "            eval_metrics.append(difference)\n",
    "\n",
    "    return predictions, losses, eval_metrics, input_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda58e54",
   "metadata": {},
   "source": [
    "## Data \n",
    "\n",
    "Now we can train our autoencoder on a dataset. The dataset we use consists of \"noise\" data which can have different variances and different means. We are going to train our machine learning model on data with 1 specific mean. Then we will evaluate our machine learning model on a time series which contains data with all the different means and variances.\n",
    "\n",
    "Before we can do this, however, we need to load and prepare our dataset. using the functions in the Data_utils_ae.py file, create a \"create_data_train\" function which has the length of the input samples and the batch size as input and outputs a train, validation and test dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c3276b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_train(seq_len, batch_size):\n",
    "    [create_function]\n",
    "    return dataloader_train, dataloader_val, dataloader_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5efe24",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "Next, choose your hyperparameters for the autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f890811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parameters():\n",
    "    learning_rate = [fill_in]\n",
    "    epochs = [fill_in]\n",
    "    batch_size = [fill_in]\n",
    "    seq_len = [fill_in]\n",
    "\n",
    "    train_params = {\n",
    "        \"epochs\": epochs,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"batch_size\": batch_size\n",
    "    }\n",
    "    return seq_len, train_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003a74c3",
   "metadata": {},
   "source": [
    "Now you can use the created functions to create a model, train it and run predictions on the test set. Play around with your model size and hyperparameters untill you get a model that can reproduce its input data well. You can check this using the loss graph and encoder graph functions defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2079f406",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_graph(history):\n",
    "    \"\"\"Plots the training history of the machine learning model\"\"\"\n",
    "    train_loss = history['train_loss']\n",
    "    print(\"train loss: \" + str(train_loss))\n",
    "    val_loss = history['val_loss']\n",
    "    train_metric = history['train_metric']\n",
    "    val_metric = history['val_metric']\n",
    "\n",
    "    fig, axs = plt.subplots(2,1)\n",
    "\n",
    "    axs[0].plot(train_loss, label='train')\n",
    "    axs[0].plot(val_loss, label='val')\n",
    "    axs[1].plot(train_metric, label='train')\n",
    "    axs[1].plot(val_metric, label='val')\n",
    "\n",
    "    axs[0].set_xlabel(\"epochs\")\n",
    "    axs[1].set_xlabel(\"epochs\")\n",
    "\n",
    "    axs[0].set_ylabel(\"Loss\")\n",
    "    axs[1].set_ylabel(\"Metric\")\n",
    "\n",
    "    axs[0].set_title(\"Train vs. validation loss\")\n",
    "    axs[1].set_title(\"Train vs. validation metric\")\n",
    "\n",
    "    axs[0].legend()\n",
    "    fig.set_size_inches(10, 10, forward=True)\n",
    "    fig.tight_layout()\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c3ba6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder_graph( predictions, inputs, eval_metrics):\n",
    "    \"\"\" Plots the inputs of the model vs. the predictions and \n",
    "     the value of the evaluation metric of the \"predict\" function.\"\"\"\n",
    "    predictions = np.concatenate(predictions).tolist()\n",
    "    inputs = np.concatenate(inputs).tolist()\n",
    "    fig, axs = plt.subplots(2,1)\n",
    "\n",
    "    axs[0].plot(inputs, label='inputs')\n",
    "    axs[0].plot(predictions, label='predictions')\n",
    "    axs[1].plot(eval_metrics)\n",
    "\n",
    "    axs[0].set_xlabel(\"number of samples\")\n",
    "    axs[1].set_xlabel(\"number of samples\")\n",
    "\n",
    "    axs[0].set_ylabel(\"value\")\n",
    "    axs[1].set_ylabel(\"Evaluation metric\")\n",
    "\n",
    "    axs[0].set_title(\"Test predictions vs inputs curve\")\n",
    "    axs[1].set_title(\"Evaluation metric\")\n",
    "\n",
    "    axs[0].legend()\n",
    "    fig.set_size_inches(16.5, 16.5, forward=True)\n",
    "    fig.tight_layout()\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98134b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d49dd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a960aab5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e26c258",
   "metadata": {},
   "source": [
    "## The model on a new dataset\n",
    "Create a \"create_data_drift\" function which does the same for the test dataset as it did for the training dataset, without splitting it into a train, val and test set.\n",
    "Then run the model on this new dataset and look at the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de714a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_drift(seq_len, batch_size):\n",
    "    [create_function]\n",
    "    return dataloader_drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be55a40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2dc369e8",
   "metadata": {},
   "source": [
    "## Drift detection\n",
    "Adapt the predict function and autoencoder graph function so that depending on the difference between input and output drift is detected and this is shown in some way in a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa74772",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424bcfff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770a2de6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
