{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63bd97ab",
   "metadata": {},
   "source": [
    "# Tabular data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfab9ee",
   "metadata": {},
   "source": [
    "## Imports needed for data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ec668e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398ef079",
   "metadata": {},
   "source": [
    "## Data importation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702bb61c",
   "metadata": {},
   "source": [
    "For tabular data we are using a water potability dataset which can be found on [Kaggle](https://www.kaggle.com/datasets/adityakadiwal/water-potability). This dataset is located in our directory under \"datasets/water_potability\" and has a csv file format. The model you would want to train with this dataset is a classification model which devides wether you could drink the water given several parameters.</br></br>\n",
    "As in the data exploration step we first need to load our dataset into python. The easiest way to read and work with tabular data is to use the Pandas library. </br>\n",
    "To read data from a csv file we use the following command: \n",
    "\n",
    "<code> data = pd.read_csv(\"pathname/to/dataset.csv\", delimiter=\",\" , index_col=None) </code>\n",
    "\n",
    "Later you will have to save your dataset. \n",
    "\n",
    "<code>data.to_csv(\"path/to/file\", sep=\";\", index=False)</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1602e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf0babc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19951870",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6664875",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e335be7",
   "metadata": {},
   "source": [
    "### Remove the Null values\n",
    "\n",
    "An important aspect of your dataset is that it is \"clean\". This means that there are no None, Null, NaN or other values given in the dataset.</br>\n",
    "In an earlier step we have checked wether or not there are any Null values, now we are going to remove them. There are several techniques possible. The one we are going to use is using the mean value of a column to fill in the empty values. To do this you can use the following commands.\n",
    "\n",
    "<code>\n",
    "mean = dataframe.mean()\n",
    "dataframe.fillna(value, inplace=True)\n",
    "</code>\n",
    "\n",
    "When you are done, don't forget to save your new dataset to a file and version with DVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026be709",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3de67688",
   "metadata": {},
   "source": [
    "### Data transformation\n",
    "When exploring the dataset we also looked at the different features and their ranges. If some of features had very different ranges it might be a good idea to normalize your dataset. Again there are several techniques to normalize your data. We are going to use the min-max normalization. To be able to do this you will need the following functions.\n",
    "</br></br>\n",
    "To iterate over the columns in your dataset: </br>\n",
    "<code>for column in dataframe.columns: </code>\n",
    "\n",
    "To find the min or the max of a column: </br>\n",
    "<code>dataframe[\"column\"].min() </br> dataframe[\"column\"].max()</code>\n",
    "\n",
    "The formula for min-max normalization is the following: </br>\n",
    "$$Xnorm = \\frac{X-Xmin}{Xmax - Xmin}$$\n",
    "\n",
    "Again, do not forget to save your new dataset version with DVC when you are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4854dbbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4bee602",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "In the previous exercise you found that one of the two Hardness columns was not real. Now it is time to remove this column since it does not give useful information to our machine learning model. This can very simply be done using the following pandas function.\n",
    "\n",
    "<code>dataframe.drop([\"column_1\", \"column_2\"], axis=1)</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3feb05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27faa8c7",
   "metadata": {},
   "source": [
    "The last step is to split your data into a training set, validation set and test set. To do this you can use the following command from scikit-learn. </br>\n",
    "\n",
    "<code>train, rest = train_test_split(dataframe, test_size=0.2)</code>\n",
    "\n",
    "A typical split is 80 percent training, 10 percent validation, 10 percent test set. \n",
    "</br>When you are done, save your three datasets as csv files. \n",
    "\n",
    "<code>dataframe.to_csv(\"path/to/save/file.csv\", sep=\";\", index=0) </code>\n",
    "\n",
    "Again, do not forget to save your new datasets with DVC when you are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158b7c72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983bb2ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81feead8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
