{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa02df28",
   "metadata": {},
   "source": [
    "# Image data preprocessing\n",
    "\n",
    "For the image dataset we are working on a waste recycling plant dataset, which can be found on [Kaggle](https://www.kaggle.com/datasets/parohod/warp-waste-recycling-plant-dataset). There are three different versions of this dataset. We are using WaRP-C which contains cutout images of a single waste object. \n",
    "In this notebook we are going to process our dataset so that it can be used for machine learning classification. The first step is to import the libraries. You can also find a function which takes a dataset as imput and plots 20 random images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "282c86d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision.transforms import v2\n",
    "from torch.utils.data import WeightedRandomSampler, Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "f4fe0480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_20_images(dataset):\n",
    "    df_sample = dataset.sample(n=20)\n",
    "    print(df_sample)\n",
    "    fig, axes = plt.subplots(nrows=4, ncols=5, figsize=(10,10))\n",
    "    count = 0\n",
    "    for index, row in df_sample.iterrows():\n",
    "        if count < 5:\n",
    "            axes[0,count].imshow(row[\"image\"])\n",
    "        elif count < 10:\n",
    "            axes[1, count-5].imshow(row[\"image\"])\n",
    "        elif count < 15:\n",
    "            axes[2, count-10].imshow(row[\"image\"])\n",
    "        else:\n",
    "            axes[3, count-15].imshow(row[\"image\"])\n",
    "        count += 1\n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd6c760",
   "metadata": {},
   "source": [
    "## Loading the image dataset\n",
    "Like before the first step is to load our image dataset into memory. We wil again use PIL Image to do this. However, this time we are only saving the image, its path and their label into the pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df09b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root dir: datasets/Warp-C/\n",
      "root dir: datasets/Warp-C/bottle-blue\n",
      "root dir: datasets/Warp-C/bottle-blue-full\n",
      "root dir: datasets/Warp-C/bottle-blue5l\n",
      "root dir: datasets/Warp-C/bottle-blue5l-full\n",
      "root dir: datasets/Warp-C/bottle-dark\n",
      "root dir: datasets/Warp-C/bottle-dark-full\n",
      "root dir: datasets/Warp-C/bottle-green\n",
      "root dir: datasets/Warp-C/bottle-green-full\n",
      "root dir: datasets/Warp-C/bottle-milk\n",
      "root dir: datasets/Warp-C/bottle-milk-full\n",
      "root dir: datasets/Warp-C/bottle-multicolor\n",
      "root dir: datasets/Warp-C/bottle-multicolor-full\n",
      "root dir: datasets/Warp-C/bottle-oil\n",
      "root dir: datasets/Warp-C/bottle-oil-full\n",
      "root dir: datasets/Warp-C/bottle-transp\n",
      "root dir: datasets/Warp-C/bottle-transp-full\n",
      "root dir: datasets/Warp-C/bottle-yogurt\n",
      "root dir: datasets/Warp-C/canister\n",
      "root dir: datasets/Warp-C/cans\n",
      "root dir: datasets/Warp-C/cardboard-juice\n",
      "root dir: datasets/Warp-C/cardboard-milk\n",
      "root dir: datasets/Warp-C/detergent-box\n",
      "root dir: datasets/Warp-C/detergent-color\n",
      "root dir: datasets/Warp-C/detergent-transp\n",
      "root dir: datasets/Warp-C/detergent-white\n",
      "root dir: datasets/Warp-C/glass-dark\n",
      "root dir: datasets/Warp-C/glass-green\n",
      "root dir: datasets/Warp-C/glass-transp\n",
      "file count: 10016\n",
      "              label                                              image\n",
      "0       bottle-blue  <PIL.JpegImagePlugin.JpegImageFile image mode=...\n",
      "1       bottle-blue  <PIL.JpegImagePlugin.JpegImageFile image mode=...\n",
      "2       bottle-blue  <PIL.JpegImagePlugin.JpegImageFile image mode=...\n",
      "3       bottle-blue  <PIL.JpegImagePlugin.JpegImageFile image mode=...\n",
      "4       bottle-blue  <PIL.JpegImagePlugin.JpegImageFile image mode=...\n",
      "...             ...                                                ...\n",
      "10011  glass-transp  <PIL.JpegImagePlugin.JpegImageFile image mode=...\n",
      "10012  glass-transp  <PIL.JpegImagePlugin.JpegImageFile image mode=...\n",
      "10013  glass-transp  <PIL.JpegImagePlugin.JpegImageFile image mode=...\n",
      "10014  glass-transp  <PIL.JpegImagePlugin.JpegImageFile image mode=...\n",
      "10015  glass-transp  <PIL.JpegImagePlugin.JpegImageFile image mode=...\n",
      "\n",
      "[10016 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "images = pd.DataFrame(columns=[\"label\", \"path\", \"image\"])\n",
    "directory = 'datasets/Warp-C/'\n",
    "for root_dir, cur_dir, files in os.walk(directory):\n",
    "    print(\"root dir: \" + str(root_dir))\n",
    "    label = os.path.basename(os.path.normpath(root_dir))\n",
    "    for file in files:\n",
    "        if \".jpg\" in file:\n",
    "            file_name = root_dir +\"/\"+ file\n",
    "            count += 1\n",
    "            image = Image.open(file_name)\n",
    "            row = [ label, image, file_name]\n",
    "            images.loc[len(images)] = row\n",
    "\n",
    "print(\"file count: \" + str(count))\n",
    "print(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e8ee10",
   "metadata": {},
   "source": [
    "Before we start processing our data we need to have a function which saves all processed images as a new dataset. Do this using the following code:\n",
    "\n",
    "    save image:\n",
    "    PIL_image.save(\"path/to/file.png\")\n",
    "\n",
    "    iterate over pandas dataframe:\n",
    "    for idx, row in result.iterrows():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "35996ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(dataframe):\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09706f80",
   "metadata": {},
   "source": [
    "## Data transformation\n",
    "\n",
    "### resizing your images\n",
    "The first step for images is to resize them. A machine learning model has a set input size, which means that all input images need to have the same dimensions. \n",
    "\n",
    "To resize an image you can use the following PIL Image function\n",
    "<code>resized_image = image.resize((size_width, size_height))</code>\n",
    "\n",
    "You will also have to work with a lambda expression. This means that you define a function (here a \"resize\" function) which you than call in the \"apply\" function on your dataframe so that each row in your dataframe will apply this function.\n",
    "\n",
    "<code>\n",
    "dataframe[\"image\"] = dataframe[\"image\"].apply(lambda img:function(inputs))\n",
    "</code>\n",
    "</br>\n",
    "You can use the plot_20_images(dataset) function to look at your results.\n",
    "\n",
    "Don't forget to save your image dataset with DVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd8bf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(image):\n",
    "\n",
    "    return resized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31139db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba6b1e51",
   "metadata": {},
   "source": [
    "### Data augmentation\n",
    "\n",
    "When creating your image dataset, you might have taken a certain amount of pictures with a specific lighting setup or a specific camera angle. However, most of the time you want your model to be robust against different lighting setups, angles and noise. To help with this, you can use data augmentation and transform techniques.\n",
    "\n",
    "Transformations allow you to change your original image in a specific way. This can include things like cropping, changing colors, adding noice, rotating or flipping the image, and many more. Pytorch has an easy way to add transformed images to your dataset:\n",
    "\n",
    "    transforms = v2.Compose([\n",
    "\n",
    "        v2.transformation1(),\n",
    "        v2.transformation2(),\n",
    "        v2.transformation3(),\n",
    "    ])\n",
    "\n",
    "    image = transforms(image)\n",
    "\n",
    "\n",
    "You can find a list of the possible transformations [here](https://docs.pytorch.org/vision/stable/transforms.html). Try to implement at least 3 different transformations to the dataset. Do this by creating a transform() function and implementing it on all images in the dataframe using a lambda expression. If you want to create a larger dataset, you can add the transformed images to the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "6314e9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(image): \n",
    "    \n",
    "    return image_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903b6da9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2196f663",
   "metadata": {},
   "source": [
    "###  Normalizing your pixel values\n",
    "\n",
    "Normally your data is saved as three matrices containing values between 0 and 255 (RGB values). However, sometimes, to create a consistent scale in your dataset it is a good idea to normalize these values between 0 and 1. We will use a transform function from torchvision. The Normalize transform function uses the mean and standard deviation to calculate the normalized values. Therefore we first need to transform our PIL image to a Tensor image consisting of floats (this is also necessary for certain other transformations). Then we can calculate the mean and standard deviation. Lastly we transform our tensor image with the Normalize function. To implement this to all images, you can again use a lambda function. You will need the following code:\n",
    "\n",
    "<code>transform = v2.Compose([<i>list, of, transformations</i>])</code></br>\n",
    "\n",
    "    result = transform(image)\n",
    "    \n",
    "    mean = image_tensor.mean([1,2])\n",
    "    std = image_tensor.std([1,2])\n",
    "\n",
    "    change int to float: v2.ToDtype(torch.float32, scale=True)\n",
    "    normalize: v2.Normalize(mean, std)\n",
    "    convert PIL to tensor: v2.ToImage()\n",
    "    Convert tensor to PIL: v2.ToPILImage()\n",
    "\n",
    "Don't forget to save your dataset with DVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "f96a610a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_image(image):\n",
    "\n",
    "    return normalized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a3b2c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da410b84",
   "metadata": {},
   "source": [
    "### Dealing with class imbalance\n",
    "\n",
    "In the original dataset we checked the amount of values per class. This is done to find out if there is class imbalance. If there is one class which has a lot of data samples and another which has very little data samples, there might be some problems whilst training. For example, you might find that the model learns to return the overrepresented class instead of actually learning something of value. This problem can be solved when creating the dataloader, which will call your data whilst training.\n",
    "\n",
    "There are several ways to deal with class imbalance. This includes oversampling (adding some samples multiple times from the unerrepresented class), undersampling (not adding some samples from the overrepresented class) or class weighting (sample the data with weigths respresenting the class imbalance). These techniques are sometimes done before training, like we will do here in the dataloader. However you can also add weigths to certain loss functions in pytorch which deal with class imbalances. \n",
    "\n",
    "In this exercise we are going to use a weighted random sampler in a dataloader. This means that we will create a torch dataset and dataloader and add the sampler to the dataloader. That way the data that the model will get whilst training will be sampled using the weigths of the class imbalances. \n",
    "\n",
    "First, you will need to calculate the class weights. The class weigths are calculated as follows: $\\frac{1}{class-count}$. This means that we first need to count the number of times a certain class is represented. Do this for the image dataset.\n",
    "\n",
    "    count = dataset[\"column\"].value_counts().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076f7ab7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb7414a4",
   "metadata": {},
   "source": [
    "When you have your clas weigths, they can be used in our WeightedRandomSampler. The weigths need to be transformed to tensors.\n",
    "\n",
    "    samples_weigths = torch.from_numpy(np.array(weigths))\n",
    "    sampler = WeightedRandomSampler(samples_weigths, len(samples_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716b7a39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5bed2ad",
   "metadata": {},
   "source": [
    "Lastly we create a TensorDataset and DataLoader from our pandas dataframe. to represent your labels as a one-hot-encoding we created a function for you. This is to change the labels from a unique string label to something that the machine learning model will understand, a vector of zeroes and ones. We also made a custom torch Dataset to be able to use in the Dataloader since we use PIL images instead of tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "8784af56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_onehotlabels(images_normal):\n",
    "    result = pd.get_dummies(images_normal[\"label\"])\n",
    "    onehotlist = []\n",
    "    for idxm, row in result.iterrows():\n",
    "        boollist= row[1:].tolist()\n",
    "        onehotlist.append([int(value) for value in boollist])\n",
    "\n",
    "    images_normal[\"label\"] = onehotlist\n",
    "\n",
    "    return images_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5574c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageLabelDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "        self.transformer = v2.ToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.transformer(self.dataframe.iloc[idx]['image'])\n",
    "        label = self.dataframe.iloc[idx]['label']\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cce2ce",
   "metadata": {},
   "source": [
    "Use all previously defined functions and classes to create a dataloader which will load the image and its one hot encoded label:\n",
    "\n",
    "    dataset = ImageLabelDataset(images)\n",
    "    dataloader = DataLoader(dataset, batch_size, sampler=sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0924c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eee684c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7b6ce0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
