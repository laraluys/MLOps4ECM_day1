{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63bd97ab",
   "metadata": {},
   "source": [
    "# Tabular data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfab9ee",
   "metadata": {},
   "source": [
    "## Imports needed for data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ec668e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398ef079",
   "metadata": {},
   "source": [
    "## Data importation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702bb61c",
   "metadata": {},
   "source": [
    "For tabular data we are using a water potability dataset which can be found on [Kaggle](https://www.kaggle.com/datasets/adityakadiwal/water-potability). This dataset is located in our directory under \"datasets/water_potability\" and has a csv file format.</br></br>\n",
    "As in the data explorationstep we first need to load our dataset into python. The easiest way to read and work with tabular data is to use the Pandas library. </br>\n",
    "To read data from a csv file we use the following command: \n",
    "\n",
    "<code> data = pd.read_csv(\"pathname/to/dataset.csv\", delimiter=\",\" , index_col=None) </code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19951870",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"datasets/water_potability/water_potability.csv\", delimiter=\";\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6664875",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e335be7",
   "metadata": {},
   "source": [
    "### Remove the Null values\n",
    "\n",
    "An important aspect of your dataset is that it is \"clean\". This means that there are no None, Null, NaN or other values given in the dataset.</br>\n",
    "In an earlier step we have checked wether or not there are any Null values, now we are going to remove them. There are several techniques possible. The one we are going to use is using the mean value of a column to fill in the empty values. To do this you can use the following commands.\n",
    "\n",
    "<code>\n",
    "mean = dataframe.mean()\n",
    "dataframe.fillna(value, inplace=True)\n",
    "</code>\n",
    "\n",
    "When you are done, don't forget to save your new dataset version with DVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "026be709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            ph        Solids  Chloramines     Sulfate  Conductivity  \\\n",
      "0     7.080795  20791.318981     7.300212  368.516441    564.308654   \n",
      "1     3.716080  18630.057858     6.635246  333.775777    592.885359   \n",
      "2     8.099124  19909.541732     9.275884  333.775777    418.606213   \n",
      "3     8.316766  22018.417441     8.059332  356.886136    363.266516   \n",
      "4     9.092223  17978.986339     6.546600  310.135738    398.410813   \n",
      "...        ...           ...          ...         ...           ...   \n",
      "3271  4.668102  47580.991603     7.166639  359.948574    526.424171   \n",
      "3272  7.808856  17329.802160     8.061362  333.775777    392.449580   \n",
      "3273  9.419510  33155.578218     7.350233  333.775777    432.044783   \n",
      "3274  5.126763  11983.869376     6.303357  333.775777    402.883113   \n",
      "3275  7.874671  17404.177061     7.509306  333.775777    327.459760   \n",
      "\n",
      "      Organic_carbon  Trihalomethanes  Turbidity  Hardness_2  Hardness_1  \\\n",
      "0          10.379783        86.990970   2.963135  204.890455 -152.488808   \n",
      "1          15.180013        56.329076   4.500656  129.422921 -152.488808   \n",
      "2          16.868637        66.420093   3.055934  224.236259 -152.488808   \n",
      "3          18.436524       100.341674   4.628771  214.373394 -161.809536   \n",
      "4          11.558279        31.997993   4.075075  181.101509 -136.883422   \n",
      "...              ...              ...        ...         ...         ...   \n",
      "3271       13.894419        66.687695   4.435821  193.681735 -170.638084   \n",
      "3272       19.903225        66.396293   2.798243  193.553212 -152.488808   \n",
      "3273       11.039070        69.845400   3.298875  175.762646 -152.488808   \n",
      "3274       11.168946        77.488213   4.708658  230.603758 -152.488808   \n",
      "3275       16.140368        78.698446   2.309149  195.102299 -152.488808   \n",
      "\n",
      "      Potability  \n",
      "0              0  \n",
      "1              0  \n",
      "2              0  \n",
      "3              0  \n",
      "4              0  \n",
      "...          ...  \n",
      "3271           1  \n",
      "3272           1  \n",
      "3273           1  \n",
      "3274           1  \n",
      "3275           1  \n",
      "\n",
      "[3276 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "mean = data.mean()\n",
    "data.fillna(mean,inplace=True)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de67688",
   "metadata": {},
   "source": [
    "### Data transformation\n",
    "When exploring the dataset we also looked at the different features and their ranges. If some of features had very different ranges it might be a good idea to normalize your dataset. Again there are several techniques to normalize your data. We are going to use the min-max normalization. To be able to do this you will need the following functions.\n",
    "</br></br>\n",
    "To iterate over the columns in your dataset: </br>\n",
    "<code>for column in dataframe.columns: </code>\n",
    "\n",
    "To find the min or the max of a column: </br>\n",
    "<code>dataframe[\"column\"].min() </br> dataframe[\"column\"].max()</code>\n",
    "\n",
    "The formula for min-max normalization is the following: </br>\n",
    "$$Xnorm = \\frac{X-Xmin}{Xmax - Xmin}$$\n",
    "\n",
    "Again, do not forget to save your new dataset version with DVC when you are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4854dbbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            ph    Solids  Chloramines   Sulfate  Conductivity  Organic_carbon  \\\n",
      "0     0.505771  0.336096     0.543891  0.680385      0.669439        0.313402   \n",
      "1     0.265434  0.300611     0.491839  0.581699      0.719411        0.497319   \n",
      "2     0.578509  0.321619     0.698543  0.581699      0.414652        0.562017   \n",
      "3     0.594055  0.356244     0.603314  0.647347      0.317880        0.622089   \n",
      "4     0.649445  0.289922     0.484900  0.514545      0.379337        0.358555   \n",
      "...        ...       ...          ...       ...           ...             ...   \n",
      "3271  0.333436  0.775947     0.533436  0.656047      0.603192        0.448062   \n",
      "3272  0.557775  0.279263     0.603473  0.581699      0.368912        0.678284   \n",
      "3273  0.672822  0.539101     0.547807  0.581699      0.438152        0.338662   \n",
      "3274  0.366197  0.191490     0.465860  0.581699      0.387157        0.343638   \n",
      "3275  0.562477  0.280484     0.560259  0.581699      0.255266        0.534114   \n",
      "\n",
      "      Trihalomethanes  Turbidity  Hardness_2  Hardness_1  Potability  \n",
      "0            0.699753   0.286091    0.571139    0.423927         0.0  \n",
      "1            0.450999   0.576793    0.297400    0.423927         0.0  \n",
      "2            0.532866   0.303637    0.641311    0.423927         0.0  \n",
      "3            0.808065   0.601015    0.605536    0.373212         0.0  \n",
      "4            0.253606   0.496327    0.484851    0.508839         0.0  \n",
      "...               ...        ...         ...         ...         ...  \n",
      "3271         0.535037   0.564534    0.530482    0.325174         1.0  \n",
      "3272         0.532673   0.254915    0.530016    0.423927         1.0  \n",
      "3273         0.560655   0.349570    0.465486    0.423927         1.0  \n",
      "3274         0.622659   0.616120    0.664407    0.423927         1.0  \n",
      "3275         0.632478   0.162441    0.535635    0.423927         1.0  \n",
      "\n",
      "[3276 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "data_copy = data.copy()\n",
    "for column in data_copy.columns: \n",
    "        data_copy[column] = (data_copy[column] - data_copy[column].min()) / (data_copy[column].max() - data_copy[column].min())\n",
    "\n",
    "print(data_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bee602",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "In the previous exercise you found that one of the two Hardness columns was not real. Now it is time to remove this column since it does not give useful information to our machine learning model. This can very simply be done using the following pandas function.\n",
    "\n",
    "<code>dataframe.drop([\"column_1\", \"column_2\"], axis=1)</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f3feb05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            ph    Solids  Chloramines   Sulfate  Conductivity  Organic_carbon  \\\n",
      "0     0.505771  0.336096     0.543891  0.680385      0.669439        0.313402   \n",
      "1     0.265434  0.300611     0.491839  0.581699      0.719411        0.497319   \n",
      "2     0.578509  0.321619     0.698543  0.581699      0.414652        0.562017   \n",
      "3     0.594055  0.356244     0.603314  0.647347      0.317880        0.622089   \n",
      "4     0.649445  0.289922     0.484900  0.514545      0.379337        0.358555   \n",
      "...        ...       ...          ...       ...           ...             ...   \n",
      "3271  0.333436  0.775947     0.533436  0.656047      0.603192        0.448062   \n",
      "3272  0.557775  0.279263     0.603473  0.581699      0.368912        0.678284   \n",
      "3273  0.672822  0.539101     0.547807  0.581699      0.438152        0.338662   \n",
      "3274  0.366197  0.191490     0.465860  0.581699      0.387157        0.343638   \n",
      "3275  0.562477  0.280484     0.560259  0.581699      0.255266        0.534114   \n",
      "\n",
      "      Trihalomethanes  Turbidity  Hardness_2  Potability  \n",
      "0            0.699753   0.286091    0.571139         0.0  \n",
      "1            0.450999   0.576793    0.297400         0.0  \n",
      "2            0.532866   0.303637    0.641311         0.0  \n",
      "3            0.808065   0.601015    0.605536         0.0  \n",
      "4            0.253606   0.496327    0.484851         0.0  \n",
      "...               ...        ...         ...         ...  \n",
      "3271         0.535037   0.564534    0.530482         1.0  \n",
      "3272         0.532673   0.254915    0.530016         1.0  \n",
      "3273         0.560655   0.349570    0.465486         1.0  \n",
      "3274         0.622659   0.616120    0.664407         1.0  \n",
      "3275         0.632478   0.162441    0.535635         1.0  \n",
      "\n",
      "[3276 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "data_copy = data_copy.drop([\"Hardness_1\"], axis=1)\n",
    "print(data_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27faa8c7",
   "metadata": {},
   "source": [
    "The last step is to split your data into a training set, validation set and test set. To do this you can use the following command from scikit-learn. </br>\n",
    "\n",
    "<code>train, test = train_test_split(dataframe, test_size=0.2)</code>\n",
    "\n",
    "A typical split is 80 percent training, 10 percent validation, 10 percent test set. \n",
    "</br>When you are done, save your three datasets as csv files. \n",
    "\n",
    "<code>dataframe.to_csv(\"path/to/save/file.csv\", sep=\";\", index=0) </code>\n",
    "\n",
    "Again, do not forget to save your new dataset version with DVC when you are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "158b7c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, rest = train_test_split(data_copy, test_size=0.2)\n",
    "val, test = train_test_split(rest, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "983bb2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"datasets/water_potability/train_set.csv\", sep=\";\", index=0)\n",
    "val.to_csv(\"datasets/water_potability/val_set.csv\", sep=\";\", index=0)\n",
    "test.to_csv(\"datasets/water_potability/test_set.csv\", sep=\";\", index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81feead8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4f8b12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
